{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FederatedLearning_Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDW1lGz0JLPx",
        "colab_type": "text"
      },
      "source": [
        "#FEDERATED LEARNING FINAL PROJECT\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfsSUzsHHhcw",
        "colab_type": "text"
      },
      "source": [
        "**Author** \n",
        ": Ateniola Oluwatobi Victor\n",
        "\n",
        "**Objective** : My implementation of the Final project in the federated Learning section of the Secure and Private AI Scholarship Challenge Nanodegree Program using the MNIST digit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gXPFf6zVT61",
        "colab_type": "code",
        "outputId": "9db63e89-d366-4bd7-9429-5ece7fc28516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/2e/16bdefc78eb089e1efa9704c33b8f76f035a30dc935bedd7cbb22f6dabaa/syft-0.1.21a1-py3-none-any.whl (219kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 2.8MB/s \n",
            "\u001b[?25hCollecting lz4>=2.1.6 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Collecting zstd>=1.4.0.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/37/6a7ba746ebddbd6cd06de84367515d6bc239acd94fb3e0b1c85788176ca2/zstd-1.4.1.0.tar.gz (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.0MB/s \n",
            "\u001b[?25hCollecting flask-socketio>=3.3.2 (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/33/31/f779e69e59f528684d8c9925b3c82a9303d148655d9671ba2975ab8c3894/Flask_SocketIO-4.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Collecting tf-encrypted>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/ff/7dbd5fc77fcec0df1798268a6b72a2ab0150b854761bc39c77d566798f0b/tf_encrypted-0.5.7-py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 37.4MB/s \n",
            "\u001b[?25hCollecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Collecting websocket-client>=0.56.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Collecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/4b/ad228451b1c071c5c52616b7d4298ebcfcac5ae8515ede959db19e4cd56d/websockets-8.0.2-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 27.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Collecting python-socketio>=4.3.0 (from flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/5a/9429c1fbc399b6079725150a36491efd6bd4691c11110f5a57e8c991de96/python_socketio-4.3.0-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 20.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 32.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Collecting python-engineio>=3.9.0 (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/44/c9f9e76463eb2793ed866459b5b4cffdb5f4a60601980d14b45954324a7b/python_engineio-3.9.1-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zstd: filename=zstd-1.4.1.0-cp36-cp36m-linux_x86_64.whl size=1067079 sha256=101cdc946fa7845c1159488fb8fcdecb29535a086a9ad8c32a65b8684913d58c\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/3f/ee/ac08c81af7c1b24a80c746df669ea3cb37542d27877d66ccf4\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44105 sha256=a93adf138b042d1622a9b77053d27b5980f7afc78a0495399536645968427f9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: lz4, zstd, python-engineio, python-socketio, flask-socketio, pyyaml, tf-encrypted, msgpack, websocket-client, websockets, syft\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "Successfully installed flask-socketio-4.2.0 lz4-2.1.10 msgpack-0.6.1 python-engineio-3.9.1 python-socketio-4.3.0 pyyaml-5.1.2 syft-0.1.21a1 tf-encrypted-0.5.7 websocket-client-0.56.0 websockets-8.0.2 zstd-1.4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWEaezUuWKaC",
        "colab_type": "code",
        "outputId": "02f28cd3-0adb-49c3-b545-8d70e0c32b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Subset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import syft as sy\n",
        "import helper\n",
        "\n",
        "#Hooking syft to torch\n",
        "hook = sy.TorchHook(th)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0803 09:58:33.568226 139995687696256 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0803 09:58:33.588890 139995687696256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGQj7JfPZSil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to create 10 virtual workers and move to a list of workers\n",
        "def create_workers():\n",
        "  workers = []\n",
        "  cartman = sy.VirtualWorker(hook, id = \"cartman\")\n",
        "  workers.append(cartman)\n",
        "  kyle = sy.VirtualWorker(hook, id = \"kyle\")\n",
        "  workers.append(kyle)\n",
        "  kenny = sy.VirtualWorker(hook, id = \"kenny\")\n",
        "  workers.append(kenny)\n",
        "  stan = sy.VirtualWorker(hook, id = \"stan\")\n",
        "  workers.append(stan)\n",
        "  butters = sy.VirtualWorker(hook, id = \"butters\")\n",
        "  workers.append(butters)\n",
        "  wendy = sy.VirtualWorker(hook, id = \"wendy\")\n",
        "  workers.append(wendy)\n",
        "  heidi = sy.VirtualWorker(hook, id = \"heidi\")\n",
        "  workers.append(heidi)\n",
        "  bebe = sy.VirtualWorker(hook, id = \"bebe\")\n",
        "  workers.append(bebe)\n",
        "  nichole = sy.VirtualWorker(hook, id = \"nichole\")\n",
        "  workers.append(nichole)\n",
        "  patty = sy.VirtualWorker(hook, id = \"patty\")\n",
        "  workers.append(patty)\n",
        "  return workers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_vP_5qrFfpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to clear out every tensor stored in the list of virtual workers\n",
        "def clear_workers(workers):\n",
        "  for worker in workers:\n",
        "    worker.clear_objects()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpF-MD4EW_ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to split the mnist test dataset into the various workers and also to load the mnist test dataset into a test loader\n",
        "def create_federated_and_test_loaders(workers, trainset, testset):\n",
        "  federated_train_loader = sy.FederatedDataLoader(\n",
        "      trainset.federate(workers), \n",
        "      batch_size=32, shuffle=True)\n",
        "\n",
        "  test_loader = th.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "  return federated_train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8xAwKFm9Vp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to train models on the virtual workers without moving any gradient to the central models until the gradients have been coallated.\n",
        "def create_train_federated_models(workers, loader, lr = 0.12, epoch = 5):\n",
        "  #sends model to first virtual worker\n",
        "  virtual_model = classifier().send(workers[0])\n",
        "  optimizer = optim.SGD(virtual_model.parameters(), lr)\n",
        "  criterion = nn.NLLLoss()\n",
        "  for n in range(epoch):\n",
        "    \n",
        "    #Integer to keep up with first index.\n",
        "    i = 0\n",
        "    \n",
        "    #Integer to keep up with current worker while training\n",
        "    j = 0\n",
        "    \n",
        "    #Integer to count number of mini-batches per worker\n",
        "    n_mbatch = 0\n",
        "    \n",
        "    #Variable to keep up with current worker while looping\n",
        "    dbLoc = None\n",
        "    \n",
        "    #Variable to store cummulative loss.\n",
        "    cum_loss = 0\n",
        "    \n",
        "    \n",
        "    for batch_idx, (imgs, labels) in enumerate(loader):\n",
        "      \n",
        "      #condition to set dbLoc to the first worker\n",
        "      if i == 0:\n",
        "        i = 2\n",
        "        dbLoc = imgs.location\n",
        "        \n",
        "      #condition to change dbLoc if img is stored on a different worker and also calculate loss\n",
        "      if dbLoc is not imgs.location:\n",
        "        print(\"The total loss for {0} for epoch {2} is {1}\".format(workers[j].id, cum_loss / n_mbatch, n+1))\n",
        "        dbLoc = imgs.location\n",
        "        j += 1\n",
        "        \n",
        "        #Moving the model to a new worker\n",
        "        virtual_model.move(dbLoc)\n",
        "        \n",
        "        #Resetting the cummulative loss and batch count to zero for new worker\n",
        "        cum_loss = 0\n",
        "        n_mbatch = 0\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = virtual_model.forward(imgs)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      cum_loss +=  loss.get().item()\n",
        "      n_mbatch += 1\n",
        "    print(\"The total loss for {0} is {1}\".format(workers[j].id, cum_loss / n_mbatch))\n",
        "    \n",
        "    #Moving the model to the first worker if training would occur again\n",
        "    if (n < (epoch - 1)):\n",
        "      virtual_model.move(workers[0])\n",
        "  return virtual_model\n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4FD2KugTeSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to return the model to the central machine\n",
        "def create_central_model(model):\n",
        "  return model.get()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdzoIdyZUOQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to analyze the private database with the trained model\n",
        "def analyze_model(model, loader):\n",
        "  print(\"Running on \", \"cpu\")\n",
        "  cum_perc = 0\n",
        "  for imgs, labels in loader:\n",
        "    with th.no_grad():\n",
        "      ps =  th.exp(model.forward(imgs))\n",
        "    top_p, top_class = ps.topk(1, dim = 1)\n",
        "    prob = top_class == labels.view(*top_class.shape)\n",
        "    prob = prob.float()\n",
        "    cum_perc += prob.mean().float()\n",
        "  print(\"The accuracy of the model is {0}%\".format((cum_perc / len(loader)) * 100))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fA1Ox2FexyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Classifier for creating the models\n",
        "class classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 32)\n",
        "    self.fc5 = nn.Linear(32, 10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = F.log_softmax(self.fc5(x), dim = 1)   \n",
        "    return x\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UiMGYx2Wx9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Application of transforms to normalize the mnist data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy7piyUUMUzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "workers = create_workers()\n",
        "clear_workers(workers)\n",
        "federated_loader, test_loader = create_federated_and_test_loaders(workers, mnist_trainset, mnist_testset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl17zwHVWePX",
        "colab_type": "code",
        "outputId": "1d777500-1f67-4355-9cdc-591e5bdcb263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "virtual_model = create_train_federated_models(workers, federated_loader, epoch = 2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total loss for cartman for epoch 1 is 1.7833157254660383\n",
            "The total loss for kyle for epoch 1 is 0.9024252550716095\n",
            "The total loss for kenny for epoch 1 is 0.6204849152647435\n",
            "The total loss for stan for epoch 1 is 0.4085380588757231\n",
            "The total loss for butters for epoch 1 is 0.3886932226571631\n",
            "The total loss for wendy for epoch 1 is 0.35908689480671224\n",
            "The total loss for heidi for epoch 1 is 0.3309765687172717\n",
            "The total loss for bebe for epoch 1 is 0.29516828555534497\n",
            "The total loss for nichole for epoch 1 is 0.2855488977375183\n",
            "The total loss for patty is 0.174475033589183\n",
            "The total loss for cartman for epoch 2 is 0.22575426379099806\n",
            "The total loss for kyle for epoch 2 is 0.20347901956832154\n",
            "The total loss for kenny for epoch 2 is 0.2153895103392449\n",
            "The total loss for stan for epoch 2 is 0.17871475322766506\n",
            "The total loss for butters for epoch 2 is 0.18308901988921014\n",
            "The total loss for wendy for epoch 2 is 0.18084254954010248\n",
            "The total loss for heidi for epoch 2 is 0.17430340308458248\n",
            "The total loss for bebe for epoch 2 is 0.17721348134998946\n",
            "The total loss for nichole for epoch 2 is 0.15758031705751063\n",
            "The total loss for patty is 0.10357205752045551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoVQPxPOT8jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "central_model = create_central_model(virtual_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpaY4ULYUbvg",
        "colab_type": "code",
        "outputId": "b2c5cb82-84fc-4c0d-ade7-7375d6887593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "analyze_model(central_model, test_loader)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on  cpu\n",
            "The accuracy of the model is 96.1783447265625%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
